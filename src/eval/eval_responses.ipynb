{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "from typing import List\n",
    "from openai import AzureOpenAI\n",
    "import re\n",
    "import numpy as np\n",
    "from call_api import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates the quality of responses using the LLM-as-a-judge method. The Judge LLM rates the responses from a score of 1 to 6 according to a rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>queries</th>\n",
       "      <th>references</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When was the Electoral Law of the National Peo...</td>\n",
       "      <td>Electoral Law of the National People's Congres...</td>\n",
       "      <td>The Electoral Law was adopted at the Second Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Who has the right to vote and stand for election?</td>\n",
       "      <td>Persons who have been deprived of political ri...</td>\n",
       "      <td>Persons who have not been deprived of politica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the purpose of the election committees?</td>\n",
       "      <td>In cities divided into districts, municipal di...</td>\n",
       "      <td>The purpose of the election committees is to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the base number of deputies to the peo...</td>\n",
       "      <td>(1) The base number of deputies to the people'...</td>\n",
       "      <td>The base number of deputies to the people's co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What determines the total number of deputies t...</td>\n",
       "      <td>The base number of deputies to a local people'...</td>\n",
       "      <td>The base number of deputies plus the number of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            queries  \\\n",
       "0           0  When was the Electoral Law of the National Peo...   \n",
       "1           1  Who has the right to vote and stand for election?   \n",
       "2           2    What is the purpose of the election committees?   \n",
       "3           3  What is the base number of deputies to the peo...   \n",
       "4           4  What determines the total number of deputies t...   \n",
       "\n",
       "                                          references  \\\n",
       "0  Electoral Law of the National People's Congres...   \n",
       "1  Persons who have been deprived of political ri...   \n",
       "2  In cities divided into districts, municipal di...   \n",
       "3  (1) The base number of deputies to the people'...   \n",
       "4  The base number of deputies to a local people'...   \n",
       "\n",
       "                                             answers  \n",
       "0  The Electoral Law was adopted at the Second Se...  \n",
       "1  Persons who have not been deprived of politica...  \n",
       "2  The purpose of the election committees is to c...  \n",
       "3  The base number of deputies to the people's co...  \n",
       "4  The base number of deputies plus the number of...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_file_path = 'test_df.csv'\n",
    "qwen_response_file = 'put response file from model here'\n",
    "data = pd.read_csv(input_file_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EVAL_PROMPT = '''### Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 6, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that critically assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 6. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 6}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "5. The responses and queries will be in English and Chinese, please ignore the differences in languages and do not penalize because of language differences. For example, if the question and the reference answers are in english but the response is in chinese, please do not deduct points from the evaluation.\n",
    "### The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "### Response to evaluate:\n",
    "{response}\n",
    "\n",
    "### Reference Answer (Score 6):\n",
    "{reference_answer}\n",
    "\n",
    "### Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual. The answer is too complicated and completely unnecessary for this question.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual. The model struggles to keep things clear.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual. The factual mistakes sometimes convey a different meaning than the answer. The model may be overly wordy, providing unnecessary or possibly misleading information.\n",
    "Score 4: The response is mostly correct, accurate, and factual. The model may have a few inaccurate places, but the key parts are all correct. The response is wordy but it does not impede overall understanding.\n",
    "Score 5: The response is completely correct, accurate, and factual. The model has little to no inaccurate mistakes. The response is mostly clear and there are little to no unrelated information in the response.\n",
    "Score 6: The response is completely correct, accurate, and factual. The model provides a clear, concise answer that is easy to understand. Only give this score if the response is perfect in all ways.\n",
    "\n",
    "Answers might be phrased differently from the reference answer, do not penalize if they have the same overall meaning.\n",
    "Judge by factualism first, and wordiness last.\n",
    "Add bonus points (the result still should be from 1 to 6) when the model keeps things simple.\n",
    "### Feedback:\n",
    "\n",
    "### 任务描述：\n",
    "给定一个指令（可能包括其中的输入）、一个需要评估的回复、一个评分为5的参考答案以及代表评分标准的评分量表。\n",
    "1. 请根据给定的评分量表严格评估回复的质量，撰写详细的反馈意见，不要进行一般性的评价。\n",
    "2. 写完反馈意见后，请给出一个1到6之间的整数分数。请参考评分量表。\n",
    "3. 输出格式应如下所示：\\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 6}}\\\"\n",
    "4. 请不要生成其他开头、结尾或解释。在输出中务必包含 [RESULT]。\n",
    "5. 回复和查询均会以英语和中文提供，请忽略语言差异，不要因语言差异而扣分。例如，如果问题和参考答案是英文的，而回复是中文的，请不要因语言差异而在评估中扣分。\n",
    "\n",
    "答案可能与参考答案的表述不同，但如果整体意义相同，请不要扣分。\n",
    "### 需要评估的指令：\n",
    "{instruction}\n",
    "\n",
    "### 需要评估的回复：\n",
    "{response}\n",
    "\n",
    "### 参考答案（评分为6）：\n",
    "{reference_answer}\n",
    "\n",
    "### 评分量表：\n",
    "[回复是否根据参考答案正确、准确且真实？]\n",
    "评分1：回复完全不正确、不准确和/或不真实。答案对于这个问题来说太复杂且完全没有必要。\n",
    "评分2：回复大部分不正确、不准确和/或不真实。模型难以保持清晰。\n",
    "评分3：回复部分正确、准确和/或真实。事实性错误有时会传达与答案不同的含义。模型可能过于冗长，提供了不必要或可能误导的信息。\n",
    "评分4：回复大部分正确、准确和真实。模型可能有一些不准确的地方，但关键部分都是正确的。回复虽然冗长，但并不妨碍整体理解。\n",
    "评分5：回复完全正确、准确和真实。模型几乎没有或完全没有不准确的错误。回复大体清晰，回复中几乎没有或完全没有不相关的信息。\n",
    "评分6：回复完全正确、准确和真实。模型提供了清晰、简洁且易于理解的答案。只有当回复在各方面都完美无缺时，才能给出这个分数。\n",
    "\n",
    "请严格进行打分。\n",
    "请优先按照准确性打分，比如有的回答过于垄长，但是结果没有问题，请不要太在意。\n",
    "### 反馈：'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen = pd.read_json(qwen_response_file)\n",
    "data_qwen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scores = []\n",
    "client = AzureOpenAI(\n",
    "    api_key=\"replace with openai API key\",\n",
    "    azure_endpoint=\"https://gptforai01.openai.azure.com/\",\n",
    "    api_version=API_VERSION\n",
    ")\n",
    "for i in tqdm(data.index, desc=f'Reading DF'):\n",
    "\n",
    "    # First we get the response from the model (Replace this part with the model you are testing)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": data['queries'][i]},\n",
    "        {\"role\": \"system\", \"content\" : \"\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    #print(messages)\n",
    "\n",
    "    #test_res_str = client.chat.completions.create(model=Deployment.GPT4O, messages=messages, max_tokens=4096).choices[0].message.content\n",
    "    test_res_str = data_qwen['response'][i]\n",
    "    #print(test_res_str)\n",
    "\n",
    "    cur_eval_prompt = EVAL_PROMPT.format(\n",
    "        instruction = data['queries'][i],\n",
    "        response = test_res_str,\n",
    "        reference_answer = data['answers'][i]\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"\"},\n",
    "        {\"role\": \"system\", \"content\" : cur_eval_prompt}\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    res = client.chat.completions.create(model=Deployment.GPT35_16K, messages=messages, max_tokens=4096)\n",
    "    resstr = res.choices[0].message.content\n",
    "\n",
    "    #print(resstr)\n",
    "\n",
    "    pattern = r'\\[RESULT\\]\\s+(\\d)'\n",
    "    # Extracting the score using the pattern\n",
    "    match = re.search(pattern, resstr)\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "        scores.append(score)\n",
    "\n",
    "        if score <= 3:\n",
    "            print(f\"BAD RESPONSE!\\nquestion:{data['queries'][i]}\\nresponse from model: {test_res_str}\\n reference answer: {data['answers'][i]}\\nfeedback: {resstr}\")\n",
    "        #if score == 6:\n",
    "        #    print(f\"PERFECT RESPONSE!\\nquestion:{data['queries'][i]}\\nresponse from model: {test_res_str}\\n reference answer: {data['answers'][i]}\\nfeedback: {resstr}\")\n",
    "        print(f\"Score: {score}, mean: {np.mean(scores)}\")\n",
    "    else:\n",
    "        print(f'response: {test_res_str}\\nanswer: {res}')\n",
    "        print(\"Score not found\")\n",
    "        \n",
    "\n",
    "\n",
    "print('--------------------')\n",
    "print(f'Average Score: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(scores)\n",
    "print(f'mean: {np.mean(scores)}')\n",
    "print(f'stdev: {np.std(scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
